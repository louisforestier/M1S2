{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP7.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JWkf2weR0V4g"},"source":["# <font color=green>APR - Travaux Pratiques n°7.</font>\n","\n","> Ce sujet est en lien avec le quatrième chapitre du cours, et concerne la programmation CUDA. Les mêmes commentaires que ceux des derniers TP s’appliquent ici aussi. \n",">\n","> Dans cette séance, l’objectif est de pratiquer la programmation CUDA, autour des patrons en temps constants (sur machine PRAM) et du patron REDUCE. *Dans tous les exercices, l’idée reste la même* : **transformer une image en lui donnant un effet par bloc de taille 32 par 32**. La couleur des pixels de chaque bloc sera unique par bloc dans l’image résultat. Elle résulte d’un calcul assez simple : la moyenne des pixels du même bloc de l’image source. Vous trouverez la version CPU dans la classe src/Exercise1/ImageBlockEffect.h.\n",">\n","> En résumé, nous allons jouer avec l’implantation de la réduction. \n",">\n","> **<font color=pink>N'oubliez d'exécuter les deux premières cellules de code afin d'installer l'extension CUDA et de vérifier son bon fonctionnement.</font>**\n","\n","## <font color=green>Installation du sous-sytème</font>"]},{"cell_type":"code","metadata":{"id":"4NuCP18Hj2kM"},"source":["# vérifions l'installation du SDK Cuda ...\n","!/usr/local/cuda/bin/nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3LoyZjGkHyw"},"source":["# Installons l'extension CUDA (n'hésitez par à aller sur la page GitHub ...)\n","!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git &> /dev/null\n","%load_ext nvcc_plugin\n","# Installons gdown pour charger fichier depuis Google Drive\n","!pip install --upgrade --no-cache-dir gdown &> /dev/null\n","# Installons g++-8\n","!sudo apt install g++-8 &> /dev/null\n","!sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 700 --slave /usr/bin/g++ g++ /usr/bin/g++-7\n","!sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 800 --slave /usr/bin/g++ g++ /usr/bin/g++-8\n","# importation Python pour charger/afficher des images\n","from google.colab.patches import cv2_imshow\n","import cv2\n","def afficher(file, width):\n","  img = cv2.imread(file)\n","  height = int(img.shape[0] * width / float(img.shape[1]))\n","  resized = cv2.resize(img, (width, height), interpolation = cv2.INTER_AREA) \n","  cv2_imshow(resized)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhLcqRBK1i_R"},"source":["---\n","# <font color=green>TP</font>\n","> L'installation s'est bien déroulée ?Parfait, maintenant au travail !\n",">\n","> En premier, il faut charger le TP7 depuis le drive Google ... Vous pouvez charger ce fichier (*i.e.* le premier, le second contient des images) sur votre ordinateur pour étudiez les interfaces, bien que la plupart soient dans le cours ...\n"]},{"cell_type":"code","metadata":{"id":"KBqMci_YlFND"},"source":["# Chargeons le TP7\n","!gdown https://drive.google.com/uc?id=1wIIk5BAYwB2WjEeZgEjCa3dVmah2VbgO\n","!gdown https://drive.google.com/uc?id=1FrHh5Pr2KlirwH6NBOrkfl-COSa9biTq\n","!unzip -oqq TP7.zip\n","!unzip -oqq Images-TP6.zip # mêmes images que le TP6 ;-)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hOU55HQNlIS4"},"source":["\n",">\n","> Le code du TP est dans le répertoire TP7. Vous pouvez le vérifier dans une cellule en tapant \" !ls TP7\" par exemple ...\n",">\n","> Nous démarrons avec l'exercice 1. \n","---\n","## <font color=green>Exercice 1</font>\n",">\n","> **Ce premier exercice se veut très simple : il s’agit de calculer la couleur de chaque bloc de l’image résultat. Pour cela, il faut reprendre la version de la réduction vue en cours, et accessible dans utils/OPP/OPP_cuda_reduce.cuh. Cela passe par un noyau qui doit :**\n","> *  **Charger le bloc en mémoire partagée.**\n","> *  **Appliquer la réduction par bloc en reprenant la fonction du cours.**\n","> *\t **En déduire la couleur du bloc et l’écrire dans l’image résultat (en remplissant le bloc, donc).**\n",">\n","> **Ici, chaque bloc de l’image est traité par un bloc de threads de taille 1024 (soit 32 warps). Cette première version est une application directe du cours (vous pouvez quand même remplacer le foncteur par l’addition classique).**\n",">\n","> **Attention : l’image est aplatie, id est les blocs de pixels sont aux adresses 1024×i.**\n",">\n",">\n","> ### <font color=green>Partie étudiante</font>\n",">\n","> La partie ci-dessous est pour vous. Répondez à l'exercice dans la cellule suivante. \n",">\n","> Pour sauvegarder, n'oubliez pas de terminer par \"Shift-Entrée\" ... \n",">\n","> **<font color=pink>Attention : ne touchez pas à la première ligne !</font>**"]},{"cell_type":"code","metadata":{"id":"EJiE3XyhoG3V"},"source":["%%cuda --name ../TP7/student/exo1/student.cu \n","#include <iostream>\n","#include <exo1/student.h>\n","#include <OPP_cuda.cuh>\n","\n","using uchar = unsigned char;\n","\n","namespace \n","{\n","\t// L'idée est de recopier le code du cours (qui est dans utils/OPP_cuda_reduce.cuh)\n","\t\n","\t// Mais, la différence est qu'ici la réduction se fait par bloc de 1024 pixels,\n","\t// un peu comme une réduction par segment, mais avec des segments implicites (chaque bloc est un segment).\n","\n","\t// Donc, il y a uniquement des réductions par blocs de pixels en utilisant threadIdx.x.\n","\n","\t// Un bloc de pixel va correspondre dans ce premier exercice à un bloc de threads (1024 dans les deux cas)\n","\n","\t//\n","\t__device__ \n","\t__forceinline__\n","\tvoid loadSharedMemory(float const*const data) \n","\t{\n","\t\t// La mémoire partagée contient des FLOAT, donc il faut changer de type\n","\t\tfloat*const shared = OPP::CUDA::getSharedMemory<float>();\n","\t\t// position dans le tableau\n","\t\tconst auto tid = threadIdx.x + blockIdx.x * blockDim.x;\n","\t\t// position dans le bloc/segment\n","\t\tshared[threadIdx.x] = data[tid]; \n","\t\t__syncthreads();\n","\t}\n","\n","\t//\n","\t__device__ \n","\t__forceinline__\n","\tvoid reduceJumpingStep(const int jump)\n","\t{\n","\t\t// TODO\n","\t}\n","\n","\t//\n","\t__device__\n","\t__forceinline__\n","\tfloat reducePerBlock(\n","\t\tfloat const*const source\n","\t) {\n","\t\t// TODO\n","\t}\n","\n","\t//\n","\t__device__\n","\t__forceinline__\n","\tvoid fillBlock(\n","\t\tconst float color, \n","\t\tfloat*const result\n","\t) {\n","\t\tconst unsigned tid = threadIdx.x + blockIdx.x * blockDim.x;\n","\t\tresult[tid] = color;\n","\t}\n","\n","\t//\n","\t__global__\n","\tvoid blockEffectKernel( \n","\t\tfloat const*const source, \n","\t\tfloat *const result\n","\t) {\n","\t\tconst float sumInBlock = reducePerBlock(source);\n","\t\tfillBlock(sumInBlock, result);\n","\t}\n","}\n","\n","// Cette fonction sera appelée trois fois pour une image donnée, car l'image est séparée en trois tableaux,\n","// l'un pour le rouge, l'autre pour le vert et enfin le dernier pour le bleu. \n","// Cela simplifie le code et réduit la pression sur les registres ;-)\n","void StudentWorkImpl::run_blockEffect(\n","\tOPP::CUDA::DeviceBuffer<float>& dev_source,\n","\tOPP::CUDA::DeviceBuffer<float>& dev_result\n",") {\n","\tconst auto size = dev_source.getNbElements();\n","\tconst auto nbWarps = 32;\n","\tdim3 threads(32*nbWarps);\n","\tdim3 blocks(( size + threads.x-1 ) / threads.x);\n","\tconst size_t sizeSharedMemory(threads.x*sizeof(float));\n","\t::blockEffectKernel<<<blocks, threads, sizeSharedMemory>>>(\n","\t\tdev_source.getDevicePointer(),\n","\t\tdev_result.getDevicePointer()\n","\t);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyNVf7VGLO82"},"source":["# exécutez cette cellule pour consulter le code du reduce \"classique\" ;-)\n","!cat TP7/utils/OPP/OPP_cuda_reduce.cuh\n","#!cat TP7/CMake*"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asx9OVL27puT"},"source":["> ### <font color=green>Compilation</font>\n","> Exécutez la cellule suivante pour compiler le code ..."]},{"cell_type":"code","metadata":{"id":"AP1UEc7h7zHg"},"source":["!cd TP7 ; sh ./build.sh exo1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-UVrk2KUaYd"},"source":["> ### <font color=green>Exécution</font>\n","> Exécutez la cellule suivante pour exécuter le code ...\n",">\n","> Pour le rapport, jouez avec la taille (pour les statistiques, cela signifie prendre des tailles importantes ...). "]},{"cell_type":"code","metadata":{"id":"R_ulfYuX-Y_F"},"source":["# launch student work\n","!./TP7/linux/exo1 -i=./Images/Flower_600x450.ppm\n","# display input image\n","print(\"\\nInput image is:\")\n","afficher(file=\"./Images/Flower_600x450.ppm\", width=600)\n","# display expected result\n","print(\"\\nExpected result is:\")\n","afficher(file=\"./Images/Flower_600x450_block_reference.ppm\", width=600)\n","# display result\n","print(\"\\nYour result is:\")\n","afficher(file=\"Images/Flower_600x450_block_student.ppm\", width = 600) \n","# width = mettez une largeur en fonction de votre bande passante Internet "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# launch student work\n","!./TP7/linux/exo1 -i=./Images/Raffael_012.ppm\n","# display result\n","afficher(\"Images/Raffael_012_block_student.ppm\", 300)"],"metadata":{"id":"OG-VkNa2xyQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# launch student work\n","!./TP7/linux/exo1 -i=./Images/asphalt-highway.ppm\n","# display result\n","afficher(\"Images/asphalt-highway_block_student.ppm\", 600)"],"metadata":{"id":"jP-M9KkBxxUH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvbHriSPmCIr"},"source":["## <font color=green>Exercice 2</font>\n","\n","> **Vous avez probablement noté que la version GPU n’est pas beaucoup plus rapide que la version CPU. Comme dit René, ce n’est pas taupe. Plusieurs raisons expliquent cela, nous allons essayer de les traiter l’une après l’autre. Dans cet exercice, l’idée est de réduire la <font color=pink>pression sur les registres</font>.**\n",">\n","> **Vous vous souvenez qu’un SMP possède un nombre fixe de registres (par exemple 64k ou 128k). Ces registres sont utilisés pour stocker les variables auto (i.e. déclarées dans vos fonctions), ainsi que les mémoires partagée et constante (les paramètres des fonctions sont en mémoire globale, en revanche). En utilisant la mémoire partagée pour la réduction, notre première version a donc requis un grand nombre de registres par bloc. Calculons cela : il vous a fallu 1024 registres pour une étape de réduction (voire 3 fois plus si jamais vous avez chargé toutes les couleurs en même temps !). Vous avez lancé 32 warps par bloc, donc 1024 threads. En supposant que chaque thread nécessite 20 registres (ce qui est très peu, en fait), alors un bloc requière 1024+20*1024 registres, soit 21k ; un SMP avec 64k registres peut alors faire tourner 3 blocs en parallèle, et pas un de plus. En supposant 32 registres maximum par thread, alors vous aurez un seul bloc par SMP ! Pas terrible pour l’efficacité !**\n",">\n","> **Ainsi, comme pour la version vue en cours, nous devons réduire le nombre de warps par bloc pour optimiser le parallélisme. <font color=pink>Utilisez-en moins dans cet exercice</font>. Pour cela il faudra modifier le fonctionnement de la réduction (en clair : dans la partie chargement il faut qu’un thread charge plusieurs valeurs, tout en respectant la propriété d’associativité ; autant ajouter ces valeurs dans un registre privé, puis charger cette somme dans la mémoire partagée …).**\n",">\n","> **<font color=pink>Attention</font> : un bloc de threads traite toujours 1024 pixels !**\n","> \n","> **Indiquez dans le rapport ce qui fonctionne le mieux sur votre GPU (indiquez lequel) en termes de taille d’un bloc (faites des essais avec l’option -w, mais attention : le schéma de calcul du reduce ne fonctionne qu’avec un nombre de warps qui est une puissance de 2 !).**\n",">\n","> **<font color=pink>NB : pensez parallèle ! Un algorithme séquentiel est inadaptable ...</font>**\n",">\n","> ### <font color=green>Partie étudiante</font>\n",">\n","> La partie ci-dessous est pour vous. Répondez à l'exercice dans la cellule suivante. \n",">\n","> Pour sauvegarder, n'oubliez pas de terminer par \"Shift-Entrée\" ... \n",">\n","> **<font color=pink>Attention : ne touchez pas à la première ligne !</font>**\n"]},{"cell_type":"code","metadata":{"id":"QwCPhpdKU8tp"},"source":["%%cuda --name ../TP7/student/exo2/student.cu\n","#include <iostream>\n","#include <exo2/student.h>\n","#include <OPP_cuda.cuh>\n","\n","using uchar = unsigned char;\n","\n","namespace \n","{\n","\t// L'opération est associative (enfin, en toute généralité), et donc les permutations de valeurs sont interdites.\n","\t// Seul les changements de parenthèses sont autorisées ...\n","\t// Donc il y a deux solutions :\n","\t// - La plus simple est d'effectuer plusieurs réductions successives par blocs\n","\t// - La plus difficile mais efficace, et de grouper les valeurs consécutives par thread.\n","\t// Avec cette seconde, le premier thread (0) va traiter des valeurs consécutives. Le thread suivant aussi, etc.\n","\t// En supposant par exemple que chaque thread traite 4 valeurs, alors les 4 premiers pixels du blocs sont utilisés par\n","\t// le thread 0, le 4 suivant par le thread 1, etc. jusqu'au thread 255 ;-)\n","\t// NB : on suppose que le nombre de warps est une puissance de 2 (et donc divise 1024)\n","\ttemplate<int NB_WARPS>\n","\t__device__ \n","\t__forceinline__\n","\tvoid loadSharedMemoryAssociate(float const*const data) \n","\t{\n","\t\tfloat*const shared = OPP::CUDA::getSharedMemory<float>();\n","\n","\t\tconst auto globalOffset = 1024 * blockIdx.x;\n","\t\tconst auto localThreadId = threadIdx.x;\n","\t\tconst unsigned nbPixelsPerThread = (1024 + 32*NB_WARPS - 1) / (32*NB_WARPS);\n","\n","\t\tfloat sumPerThread = 0.f;\n","\n","\t\tfor(unsigned i=0; i<nbPixelsPerThread; ++i) \n","\t\t{\n","\t\t\t// indice du pixel à traiter\n","\t\t\tconst auto pixelIdInBlock = nbPixelsPerThread * localThreadId + i;\n","\t\t\t\n","\t\t\t// TODO\n","\t\t}\n","\t\tshared[localThreadId] = sumPerThread;\n","\t\t__syncthreads();\n","\t}\n","\n","\n","\t// idem exo1, sauf test de débordement\n","\ttemplate<int NB_WARPS>\n","\t__device__ \n","\t__forceinline__\n","\tvoid reduceJumpingStep(const int jump)\n","\t{\n","\t\t// TODO \n","\t}\n","\n","\n","\t// on ne changera ici que le nombre d'itérations (10 avant, ici moins)\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tfloat reducePerBlock(\n","\t\tfloat const*const source\n","\t) {\n","\t\t// TODO\n","\t}\t\n","\t\n","\n","\t// ressemble beaucoup à l'exo1 ...\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tvoid fillBlock(\n","\t\tconst float color, \n","\t\tfloat*const result\n","\t) {\n","\t\t// calcul de l'offset du bloc : la taille est 1024\n","\t\tconst auto offset = blockIdx.x * 1024;\n","\t\t// TODO\n","\t}\n","\n","\n","\t// idem exo1 with templates\n","\ttemplate<int NB_WARPS>\n","\tstruct EvaluateWarpNumber {\n","\t\tenum { res = 1 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<1> {\n","\t\tenum { res = 16 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<2> {\n","\t\tenum { res = 8 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<4> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<8> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<16> {\n","\t\tenum { res = 2 };\n","\t};\n","\n","\t// idem exo1\n","\ttemplate<int NB_WARPS=32>\n","\t__global__\n","\t__launch_bounds__(32*NB_WARPS , EvaluateWarpNumber<NB_WARPS>::res)\n","\tvoid blockEffectKernel( \n","\t\tfloat const*const source, \n","\t\tfloat *const result\n","\t) {\n","\t\tconst float sumInBlock = reducePerBlock<NB_WARPS>(source);\n","\t\tfillBlock<NB_WARPS>(sumInBlock, result);\n","\t}\n","}\n","\n","\n","// idem exo1, sauf la taille d'un bloc de threads (et les templates)\n","void StudentWorkImpl::run_blockEffect(\n","\tOPP::CUDA::DeviceBuffer<float>& dev_source,\n","\tOPP::CUDA::DeviceBuffer<float>& dev_result,\n","\tconst unsigned nbWarps\n",") {\n","\t// Le nombre de warps est réduit ...\n","\tconst auto size = dev_source.getNbElements();\n","\t// Le nombre de threads par bloc dépend du nombre de warps ;-)\n","\tdim3 threads(32 * nbWarps); \n","\t// Attention : le nombre de blocs est calculer en considérant des traitements de 1024 pixels ! \n","\tdim3 blocks ((size + 1024-1) / 1024 );\n","\t// le reste est classique\n","\tconst size_t sizeSharedMemory(threads.x*sizeof(float));\n","\tswitch(nbWarps) {\n","\t\tcase 1:\n","\t\t\t::blockEffectKernel<1> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 2:\n","\t\t\t::blockEffectKernel<2> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 4:\n","\t\t\t::blockEffectKernel<4> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 8:\n","\t\t\t::blockEffectKernel<8> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 16:\n","\t\t\t::blockEffectKernel<16> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 32:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tdefault:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t}\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIHRCxc7o-a0"},"source":["> ### <font color=green>Compilation</font>\n","> Exécutez la cellule suivante pour compiler le code ..."]},{"cell_type":"code","metadata":{"id":"Oh257QTWpE9S"},"source":["!cd TP7 ; sh ./build.sh exo2\n","!ls Images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBSA6TMVpl-i"},"source":["> ### <font color=green>Exécution</font>\n","> Exécutez les trois cellules suivantes pour exécuter le code (avec les images pré-chargées) ...\n",">\n","> N'hésitez pas à jouer avec le paramètre -w=?? pour tester différents nombres de warps ..."]},{"cell_type":"code","metadata":{"id":"EJFYweIipnzF"},"source":["# launch student work (w is number of warps, you can modify it into 1..8)\n","!./TP7/linux/exo2 -i=./Images/Flower_600x450.ppm -w=8\n","# display result\n","afficher(file=\"Images/Flower_600x450_block_student.ppm\", width = 600) \n","# width = mettez une largeur en fonction de votre bande passante Internet "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTi8FfgxptDp"},"source":["# launch student work\n","!./TP7/linux/exo2 -i=./Images/Raffael_012.ppm -w=8\n","# display result\n","afficher(\"Images/Raffael_012_block_student.ppm\", 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0K9pT0ItsRgl"},"source":["# launch student work\n","!./TP7/linux/exo2 -i=./Images/asphalt-highway.ppm -w=8\n","# display result\n","afficher(\"Images/asphalt-highway_block_student.ppm\", 600)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHZtnN_Lwnqi"},"source":["## <font color=green>Exercice 3</font>\n","\n","> **Dans l’exercice 2, le respect de la propriété d’associativité a rendu le chargement de la mémoire cache un peu délicat (chargement par bloc pour un même thread). Une version plus simple consiste à charger les données par bloc pour tous les threads : un thread charge la valeur à l’indice correspondant à sa position, puis la valeur de même indice plus le nombre total de threads, puis la valeur à son indice plus le nombre total de threads fois 2, etc. Ce schéma plus efficace (en termes de coalescence) nécessite la propriété de commutativité, qui n’est pas requise dans une réduction classique (pensez au produit de matrices …). Ici, l’opération associative est une simple addition sur des réels, donc commutative. **\n",">\n","> **Modifiez dans cet exercice le chargement des données en mémoire cache.**\n",">\n","> ### <font color=green>Partie étudiante</font>\n",">\n","> La partie ci-dessous est pour vous. Répondez à l'exercice dans la cellule suivante. \n",">\n","> Pour sauvegarder, n'oubliez pas de terminer par \"Shift-Entrée\" ... \n",">\n","> **<font color=pink>Attention : ne touchez pas à la première ligne !</font>**\n"]},{"cell_type":"code","metadata":{"id":"0dzelQtXtY6V"},"source":["%%cuda --name ../TP7/student/exo3/student.cu\n","#include <iostream>\n","#include <exo3/student.h>\n","#include <OPP_cuda.cuh>\n","\n","using uchar = unsigned char;\n","\n","namespace \n","{\n","\t// Beaucoup de solutions ici sont possibles ...\n","\t// Celle-ci traite le bloc de pixels par morceau. Chaque pixel d'un morceau est traité par un thread.\n","\t// On répète le processus jusqu'à avoir couvert tout le bloc de pixel.\n","\t// NB: il faut un réduction par thread (séquentielle, avec variable privée)\n","\ttemplate<int NB_WARPS>\n","\t__device__ \n","\t__forceinline__\n","\tvoid loadSharedMemoryCommutative(float const*const data) \n","\t{\n","\t\tfloat*const shared = OPP::CUDA::getSharedMemory<float>();\n","\t\tfloat sum = 0.f;\n","\t\tconst unsigned globalOffset = blockIdx.x * 1024; \n","\t\tfor(auto tid = threadIdx.x; tid < 1024; tid += 32*NB_WARPS) \n","\t\t{\n","\t\t\t// TODO\n","\t\t}\n","\t\tconst auto localThreadId = threadIdx.x;\n","\t\tshared[localThreadId] = sum;\n","\t\t__syncthreads();\n","\t}\n","\n","\t// idem exo2\n","\ttemplate<int NB_WARPS>\n","\t__device__ \n","\t__forceinline__\n","\tvoid reduceJumpingStep(const int jump)\n","\t{\n","\t\t// TODO\n","\t}\n","\n","\n","\t// Idem exo2, sauf le nom de la fonction de chargement ;-)\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tfloat reducePerBlock(\n","\t\tfloat const*const source\n","\t) {\n","\t\t// TODO\n","\t}\t\n","\t\n","\n","\t// idem exo2\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tvoid fillBlock(\n","\t\tconst float color, \n","\t\tfloat*const result\n","\t) {\n","\t\t// TODO\n","\t}\n","\n","\n","\t// idem exo1\n","\ttemplate<int NB_WARPS>\n","\tstruct EvaluateWarpNumber {\n","\t\tenum { res = 1 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<1> {\n","\t\tenum { res = 16 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<2> {\n","\t\tenum { res = 8 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<4> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<8> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<16> {\n","\t\tenum { res = 2 };\n","\t};\n","\t\n","\ttemplate<int NB_WARPS=32>\n","\t__global__\n","\t__launch_bounds__(32*NB_WARPS , EvaluateWarpNumber<NB_WARPS>::res)\n","\tvoid blockEffectKernel( \n","\t\tfloat const*const source, \n","\t\tfloat *const result\n","\t) {\n","\t\tconst float sumInBlock = reducePerBlock<NB_WARPS>(source);\n","\t\tfillBlock<NB_WARPS>(sumInBlock, result);\n","\t}\n","}\n","\n","\n","void StudentWorkImpl::run_blockEffect(\n","\tOPP::CUDA::DeviceBuffer<float>& dev_source,\n","\tOPP::CUDA::DeviceBuffer<float>& dev_result,\n","\tconst unsigned nbWarps\n",") {\n","\tconst auto size = dev_source.getNbElements();\n","\n","\tdim3 threads(32 * nbWarps); \n","\tdim3 blocks((size +1023) / 1024);\n","\tconst size_t sizeSharedMemory(threads.x*sizeof(float));\n","\tswitch(nbWarps) {\n","\t\tcase 1:\n","\t\t\t::blockEffectKernel<1> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 2:\n","\t\t\t::blockEffectKernel<2> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 4:\n","\t\t\t::blockEffectKernel<4> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 8:\n","\t\t\t::blockEffectKernel<8> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 16:\n","\t\t\t::blockEffectKernel<16> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 32:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tdefault:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t}\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvoDexC8wGDT"},"source":["> ### <font color=green>Compilation</font>\n","> Exécutez la cellule suivante pour compiler le code ..."]},{"cell_type":"code","metadata":{"id":"SVGYZqY_wevx"},"source":["!cd TP7 ; sh ./build.sh exo3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cwF8WIwjxLKk"},"source":["> ### <font color=green>Exécution</font>\n","> Exécutez les trois cellules suivantes pour exécuter le code (avec les images pré-chargées) ..."]},{"cell_type":"code","metadata":{"id":"xuUA69WExFDo"},"source":["# launch student work (w is number of warps, you can modify it into 1..8)\n","!./TP7/linux/exo3 -i=./Images/Flower_600x450.ppm -w=1\n","# display result\n","afficher(file=\"Images/Flower_600x450_block_student.ppm\", width = 600) \n","# width = mettez une largeur en fonction de votre bande passante Internet "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjqmynVPx_Vu"},"source":["# launch student work\n","!./TP7/linux/exo3 -i=./Images/Raffael_012.ppm -w=1\n","# display result\n","afficher(\"Images/Raffael_012_block_student.ppm\", 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZywCVQEyI8H"},"source":["# launch student work\n","!./TP7/linux/exo3 -i=./Images/asphalt-highway.ppm -w=1\n","# display result\n","afficher(\"Images/asphalt-highway_block_student.ppm\", 600)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zeO6fOD4yyXZ"},"source":["## <font color=green>Exercice 4</font>\n","\n","> **Il est possible de tirer parti de la propriété de commutativité pour accélérer encore plus la réduction. Modifiez le schéma de calcul de la fonction reduceJumpingStep en ce sens, de façon à avoir moins de warps au travail.**\n","> *\t**Sur une feuille de papier, dessinez le schéma de calcul actuel (avec 4 warps de 4 threads chacun pour simplifier, c’est suffisant pour réfléchir…).** \n","> *\t**En déduire un second schéma minimisant le nombre de warps au travail, grâce à la commutation de l’addition.**\n","> *\t**Puis codez ce nouveau schéma.**\n",">\n","> ### <font color=green>Partie étudiante</font>\n",">\n","> La partie ci-dessous est pour vous. Répondez à l'exercice dans la cellule suivante. \n",">\n","> Pour sauvegarder, n'oubliez pas de terminer par \"Ctrl-Entrée\" ... \n",">\n","> **<font color=pink>Attention : ne touchez pas à la première ligne !</font>**"]},{"cell_type":"code","metadata":{"id":"-JxKmfOJyY1-"},"source":["%%cuda --name ../TP7/student/exo4/student.cu\n","#include <iostream>\n","#include <exo4/student.h>\n","#include <OPP_cuda.cuh>\n","\n","using uchar = unsigned char;\n","\n","namespace \n","{\n","\t// idem exo3\n","\ttemplate<int NB_WARPS>\n","\t__device__ \n","\t__forceinline__\n","\tvoid loadSharedMemoryCommutative(float const*const data) \n","\t{\n","\t\t// TODO\n","\t}\n","\n","\t// nouvelle version :-)\n","\t__device__ \n","\t__forceinline__\n","\tvoid reduceJumpingStep(const int jump)\n","\t{\n","\t\t// TODO\n","\t}\n","\n","\t// similaire précédente, mais boucle différente (les threads qui travaillent sont en tête ...)\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tfloat reducePerBlock(\n","\t\tfloat const*const source\n","\t) {\n","\t\t// TODO\n","\t}\n","\n","\t\n","\t// idem exo3\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tvoid fillBlock(\n","\t\tconst float color, \n","\t\tfloat*const result\n","\t) {\n","\t\t// TODO\n","\t}\n","\n","\n","\t// idem exo1\n","\ttemplate<int NB_WARPS>\n","\tstruct EvaluateWarpNumber {\n","\t\tenum { res = 1 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<1> {\n","\t\tenum { res = 16 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<2> {\n","\t\tenum { res = 8 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<4> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<8> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<16> {\n","\t\tenum { res = 2 };\n","\t};\n","\ttemplate<int NB_WARPS=32>\n","\t__global__\n","\t__launch_bounds__(32*NB_WARPS , EvaluateWarpNumber<NB_WARPS>::res)\n","\tvoid blockEffectKernel( \n","\t\tfloat const*const source, \n","\t\tfloat *const result\n","\t) {\n","\t\tconst float sumInBlock = reducePerBlock<NB_WARPS>(source);\n","\t\tfillBlock<NB_WARPS>(sumInBlock, result);\n","\t}\n","}\n","\n","\n","// Attention : ici la taille des vecteurs n'est pas toujours un multiple du nombre de threads !\n","// Il faut donc corriger l'exemple du cours ...\n","void StudentWorkImpl::run_blockEffect(\n","\tOPP::CUDA::DeviceBuffer<float>& dev_source,\n","\tOPP::CUDA::DeviceBuffer<float>& dev_result,\n","\tconst unsigned nbWarps\n",") {\n","\tconst auto size = dev_source.getNbElements();\n","\tdim3 threads( 32 * nbWarps );\n","\tdim3 blocks( (size + 1023) / 1024 );\n","\tconst size_t sizeSharedMemory(threads.x*sizeof(float));\n","\tswitch(nbWarps) {\n","\t\tcase 1:\n","\t\t\t::blockEffectKernel<1> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 2:\n","\t\t\t::blockEffectKernel<2> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 4:\n","\t\t\t::blockEffectKernel<4> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 8:\n","\t\t\t::blockEffectKernel<8> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 16:\n","\t\t\t::blockEffectKernel<16> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 32:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tdefault:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t}\n","\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0zT2GUWczFbK"},"source":["> ### <font color=green>Compilation</font>\n","> Exécutez la cellule suivante pour compiler le code ..."]},{"cell_type":"code","metadata":{"id":"2S81nJcEzD3b"},"source":["!cd TP7 ; sh ./build.sh exo4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TS9cfqN7zUy2"},"source":["> ### <font color=green>Exécution</font>\n","> Exécutez les trois cellules suivantes pour exécuter le code (avec les images pré-chargées) ..."]},{"cell_type":"code","metadata":{"id":"brbWIFLDgIuS"},"source":["# launch student work (w is number of warps, you can modify it into 1..8)\n","!./TP7/linux/exo4 -i=./Images/Flower_600x450.ppm -w=4\n","# display result\n","afficher(file=\"Images/Flower_600x450_block_student.ppm\", width = 600) \n","# width = mettez une largeur en fonction de votre bande passante Internet "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucqibhCtgIuf"},"source":["# launch student work\n","!./TP7/linux/exo4 -i=./Images/Raffael_012.ppm -w=4\n","# display result\n","afficher(\"Images/Raffael_012_block_student.ppm\", 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGxy3CUugIuu"},"source":["# launch student work\n","!./TP7/linux/exo4 -i=./Images/asphalt-highway.ppm -w=4\n","# display result\n","afficher(\"Images/asphalt-highway_block_student.ppm\", 600)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2noLsUYzrJe"},"source":["## <font color=green>Exercice 5</font>\n","\n","> **La version précédente est efficace car moins de warps travaillent. Vous avez probablement observé sur votre feuille de papier que les dernières itérations n’utilisent qu’un seul et unique warp : pour ajouter 64 valeurs, puis 32, puis 16, puis 8, puis 4, puis enfin les 2 dernières. Un seul warp pour traiter ces 6 étapes ! Nagerions-nous en plein Tolkien ? Et pourtant, vous continuez à synchroniser les threads, ce qui est inutile au sein du même warp (SIMD oblige !).** \n",">\n","> **Supprimez, pour ces dernières réductions (et uniquement celles-ci) la synchronisation. Attention : la mémoire partagée devra avoir l’attribut volatile … sinon le compilateur va chercher à optimiser avec des variables privées.**\n",">\n","> **Refaites l’étude sur le nombre de threads optimal.**\n","> \n","> **Et voilà, vous avez de quoi écrire une nouvelle version de la réduction, pour une fonction associative et commutative …**\n",">\n","> ### <font color=green>Partie étudiante</font>\n",">\n","> La partie ci-dessous est pour vous. Répondez à l'exercice dans la cellule suivante. \n",">\n","> Pour sauvegarder, n'oubliez pas de terminer par \"Shift-Entrée\" ... \n",">\n","> **<font color=pink>Attention : ne touchez pas à la première ligne !</font>**"]},{"cell_type":"code","metadata":{"id":"JXxpNyDSzost"},"source":["%%cuda --name ../TP7/student/exo5/student.cu\n","#include <iostream>\n","#include <exo5/student.h>\n","#include <OPP_cuda.cuh>\n","\n","using uchar = unsigned char;\n","\n","namespace \n","{\n","\t// idem exo3\n","\ttemplate<int NB_WARPS>\n","\t__device__ \n","\t__forceinline__\n","\tvoid loadSharedMemoryCommutative(float const*const data) \n","\t{\n","\t\t// TODO\n","\t}\n","\n","\t// idem exo4\n","\t__device__ \n","\t__forceinline__\n","\tvoid reduceJumpingStep(const int jump)\n","\t{\n","\t\t// TODO\n","\t}\n","\n","\t// nouvelle fonction !\n","\t__device__ \n","\t__forceinline__\n","\tvoid reduceLastWarp()\n","\t{\n","\t\t// attention au mot clé volatile ... essentiel !\n","\t\tvolatile float*const shared = OPP::CUDA::getSharedMemory<float>();\n","\t\tconst auto tid = threadIdx.x;\n","\t\tif( tid < 32 ) \n","\t\t{\n","\t\t\t// TODO\n","\t\t}\n","\t\t__syncthreads();\n","\t\t\n","\t}\n","\t\n","\t// \n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tfloat reducePerBlock(\n","\t\tfloat const*const source\n","\t) {\n","\t\t// TODO\n","\t}\n","\n","\t\n","\t// idem exo3\n","\ttemplate<int NB_WARPS>\n","\t__device__\n","\t__forceinline__\n","\tvoid fillBlock(\n","\t\tconst float color, \n","\t\tfloat*const result\n","\t) {\n","\t\t// TODO\n","\t}\n","\n","\n","\t// idem exo1\n","\ttemplate<int NB_WARPS>\n","\tstruct EvaluateWarpNumber {\n","\t\tenum { res = 1 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<1> {\n","\t\tenum { res = 16 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<2> {\n","\t\tenum { res = 8 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<4> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<8> {\n","\t\tenum { res = 4 };\n","\t};\n","\ttemplate<>\n","\tstruct EvaluateWarpNumber<16> {\n","\t\tenum { res = 2 };\n","\t};\n","\ttemplate<int NB_WARPS=32>\n","\t__global__\n","\t__launch_bounds__(32*NB_WARPS , EvaluateWarpNumber<NB_WARPS>::res)\n","\tvoid blockEffectKernel( \n","\t\tfloat const*const source, \n","\t\tfloat *const result\n","\t) {\n","\t\tconst float sumInBlock = reducePerBlock<NB_WARPS>(source);\n","\t\tfillBlock<NB_WARPS>(sumInBlock, result);\n","\t}\n","}\n","\n","\n","// Attention : ici la taille des vecteurs n'est pas toujours un multiple du nombre de threads !\n","// Il faut donc corriger l'exemple du cours ...\n","void StudentWorkImpl::run_blockEffect(\n","\tOPP::CUDA::DeviceBuffer<float>& dev_source,\n","\tOPP::CUDA::DeviceBuffer<float>& dev_result,\n","\tconst unsigned nbWarps\n",") {\n","\tconst auto size = dev_source.getNbElements();\n","\tdim3 threads(32*nbWarps);\n","\tdim3 blocks((size + 1023) / 1024);\n","\tconst size_t sizeSharedMemory(threads.x*sizeof(float));\n","\n","\tswitch(nbWarps) {\n","\t\tcase 1:\n","\t\t\t::blockEffectKernel<1> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 2:\n","\t\t\t::blockEffectKernel<2> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 4:\n","\t\t\t::blockEffectKernel<4> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 8:\n","\t\t\t::blockEffectKernel<8> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 16:\n","\t\t\t::blockEffectKernel<16> <<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tcase 32:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t\tdefault:\n","\t\t\t::blockEffectKernel<32><<<blocks, threads, sizeSharedMemory>>>(\n","\t\t\t\tdev_source.getDevicePointer(),\n","\t\t\t\tdev_result.getDevicePointer()\n","\t\t\t);\n","\t\t\treturn;\n","\t}\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P795htTf00eo"},"source":["> ### <font color=green>Compilation</font>\n","> Exécutez la cellule suivante pour compiler le code ..."]},{"cell_type":"code","metadata":{"id":"1WuW0YMf0zIH"},"source":["!cd TP7 ; sh ./build.sh exo5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KfKbA3bj0_01"},"source":["> ### <font color=green>Exécution</font>\n","> Exécutez les trois cellules suivantes pour exécuter le code (avec les images pré-chargées) ...\n",">\n","> Pour le rapport, jouez avec le nombre de warps (pour les statistiques). "]},{"cell_type":"code","metadata":{"id":"bFpxNs3Mgr89"},"source":["# launch student work (w is number of warps, you can modify it into 1..8)\n","!./TP7/linux/exo5 -i=./Images/Flower_600x450.ppm -w=4\n","# display result\n","afficher(file=\"Images/Flower_600x450_block_student.ppm\", width = 600) \n","# width = mettez une largeur en fonction de votre bande passante Internet "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsJOXPERgr9K"},"source":["# launch student work\n","!./TP7/linux/exo5 -i=./Images/Raffael_012.ppm -w=4\n","# display result\n","afficher(\"Images/Raffael_012_block_student.ppm\", 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruc1wTgLgr9Y"},"source":["# launch student work\n","!./TP7/linux/exo5 -i=./Images/asphalt-highway.ppm -w=4\n","# display result\n","afficher(\"Images/asphalt-highway_block_student.ppm\", 600)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZiGcbya2TwT"},"source":["# <font color=green>That's all, folks!</font>"]}]}